[
  {
    "objectID": "data_aquisition_cleaning.html",
    "href": "data_aquisition_cleaning.html",
    "title": "",
    "section": "",
    "text": "Querying, cleaning, and mapping GBIF data\n\nIn this lesson, you will learn\n\nWhat species occurrence data is\nHow to obtain publicly-available occurrence data from GBIF\nHow to plot occurrence data with ggplot\nHow to clean occurrence data\n\n\n\nWhat is species occurrence data?\nOccurrence data is a record of where a species was at a given time.\n\n\nHow can we get occurrence data?\nWe can use the Global Biodiversity Information Facility. Data on GBIF is sourced from a wide variety of places: from users on iNaturalist equipped with a smartphone to museum specimens. You will read more about the accuracy of citizen science collected occurrence data on February 14.\nFirst we need to load the necessary packages. We will use the tidyverse for data cleaning and visualization. We will use a package called spocc, which stands for “interface to SPecies OCCurence data sources.”\n\n#Install packages from cran, necessary to specify the repos on quarto but not on your local computer copy\ninstall.packages(\"spocc\", repos = \"http://cran.us.r-project.org\")\n\nInstalling package into '/Users/ameliapruiett/Library/R/x86_64/4.2/library'\n(as 'lib' is unspecified)\n\n\n\nThe downloaded binary packages are in\n    /var/folders/jw/09h6w34n55534tfy1wjbhrwr7h160v/T//RtmpaVkZuh/downloaded_packages\n\ninstall.packages(\"tidyverse\", repos = \"http://cran.us.r-project.org\") #includes ggplot\n\nInstalling package into '/Users/ameliapruiett/Library/R/x86_64/4.2/library'\n(as 'lib' is unspecified)\n\n\n\nThe downloaded binary packages are in\n    /var/folders/jw/09h6w34n55534tfy1wjbhrwr7h160v/T//RtmpaVkZuh/downloaded_packages\n\n\nLoad libraries\n\nlibrary(spocc)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the \u001b]8;;http://conflicted.r-lib.org/\u0007conflicted package\u001b]8;;\u0007 to force all conflicts to become errors\n\n\nRemember to use lots of comments on your own copy of this code, to help yourself remember later on!\nWe can directly query occurrence data in R with the “occ” function, which is part of the spocc package.Take a look at the documentation. In groups: write a query for Rana draytonii from GBIF, from any country during any time period. Limit it to 5000 occurrences.\n\nmyQuery <-occ(query=\"Rana draytonii\", from=\"gbif\", limit=5000)\nmyQuery\n\nSearched: gbif\nOccurrences - Found: 4,637, Returned: 4,637\nSearch type: Scientific\n  gbif: Rana draytonii (4637)\n\n\nGreat! Now we have our query. We need to access the actual data from GBIF. We will use the “$” to navigate the layers of this data.\n\nrana <- myQuery$gbif$data$Rana_draytonii\n\n\n\nHow can we plot occurence data simply to look for outliers\nLet’s initially plot this data on a map. It can be especially useful with species occurrence data to look at the distribution on a world map to find any outliers. We will use the map data in ggplot2 to add country lines as well. We’ll go into more detail in the next lesson about using ggplot2 for an occurrence map.\n\nwrld<-ggplot2::map_data(\"world\") # obtaining world borders\n\nggplot()+\n  geom_polygon(data=wrld, mapping=aes(x=long, y=lat, group=group), fill=\"grey75\", colour=\"grey60\")+\n  geom_point(data=rana, mapping=aes(x=longitude, y=latitude), show.legend=FALSE)+\n  labs(title=\"Species occurrences of R. draytonii\", x=\"longitude\", y=\"latitude\")\n\nWarning: Removed 1773 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\nHow can we clean our data\nNow we want to clean our data. What do we need to do based on the reading for today? We need to: remove outliers (outside of normal sp range), remove duplicates, and deal with NA values. Looking at the map, we can see there are a few points in Africa. This is likely due to a recording error of the location, as they are near (0,0). We can also remove them on the basis of pre-existing knowledge that these frogs are only found in the southwestern United States. We are going to use the pipe %>% to string together a series of commands.\n\ncleanRana <- rana %>% \n  filter(longitude < 0) %>%  # this is one way of removing the pts in Africa\n  filter(latitude != \"NA\", longitude != \"NA\") %>%  # remove places where lat or long are NA\n  filter(occurrenceStatus == \"PRESENT\") %>% \n  mutate(location = paste(latitude, longitude, dateIdentified, sep = \"/\")) %>% # remove duplicates: create a new column that states the location and time\n  distinct(location, .keep_all = TRUE) # remove duplicates: keep only one per duplicated entry\n\nOur data, cleanRana, has over 140 columns! That is a lot of information that we do not need. We are going to pick out only the columns we need.\n\ncleanRanaSubset <- cleanRana %>% \n  select(longitude, latitude, eventDate)\n\n\n\nWriting our data to a csv file\nLastly, we are going to write our clean data to a csv file so we can use it in later lessons.\n\nwrite.csv(cleanRanaSubset, \"ranaData.csv\")\n\nGreat job!"
  },
  {
    "objectID": "HW_2_pt1.html",
    "href": "HW_2_pt1.html",
    "title": "",
    "section": "",
    "text": "HW 2 (Part 1): Informational Interview\nHW 2 (Part 1): Informational Interview Instructions and Hints\n\n\nInstructions: Familiarize yourself with informational interviews by googling / chatGPTing ‘informational interview’. Suggested below is a youtube video to get you started too. During class on January 24, you will ask the USFWS folks the questions below in a small group setting. You will then summarize your perception of the interview, reflecting on what you learned with your personal course goals, and share with the others. \n\n\nYoutube Description of Informational Interviews: https://www.youtube.com/watch?v=m6Pa4ZB4mvQ\n\n\nPotential Informational Interview Questions:\n\n\n\nWhat did your journey look like to get to where you are today?\nWhat is your day-to-day life like at work?\nWhat do you enjoy most about your work?\nWhat is your biggest challenge at your job?\nWhat would you like to be spending more time doing at your job?\nWhat skill sets are most important in your role?\nWhat professional trends or changes should I be aware of? What didn’t you know prior to your position that you wish someone had told you?\nIf you were in my shoes, what people would you be talking with?\nCan you connect me with 3 “nouns” (people/places/or things) I should be looking into?\nStudents: come up with your own questions. What do you want to know from this expert? \n\n\n\nSummarize : Reflect on the informational interview you conducted. In 1 - 2 pages, please answer\n\nWho did you interview? \nWhat was your interviewee’s path to their current position?\nWhat is their current position like (activities, challenges, skills, etc)? Do you think you would enjoy their job? Why or why not?\nWhat was one thing that surprised you about your informational interview?\nWhat “nouns” (Q8) did your interviewee connect you with? Are you interested in them? Have you followed up with them? \nHave you ever conducted an informational interview? How was this process for you in general? \n\n\n\n\nFor those of you looking for extra challenges in this exercise\n\n\nProfessional Challenge: Do informational interviews with 2 other people the USFWS folks suggest you chat with next\n\n\nCoding Challenge: Make your informational interview summary as a markdown file"
  },
  {
    "objectID": "sdm_future.html#note-predictextent-is-defined-in-sdm_maxent_lesson.r",
    "href": "sdm_future.html#note-predictextent-is-defined-in-sdm_maxent_lesson.r",
    "title": "",
    "section": "NOTE: predictExtent is defined in sdm_maxent_lesson.R",
    "text": "NOTE: predictExtent is defined in sdm_maxent_lesson.R"
  },
  {
    "objectID": "HW_2_pt2.html",
    "href": "HW_2_pt2.html",
    "title": "",
    "section": "",
    "text": "HW 2 (Part 2): Resume\nResume Instructions and Hints\n\n\nBackground: A resume is a list of knowledge, skills, and abilities for that particular job.  Or how will you be able to do the job advertised? Thus each resume is tailored to the job posting. So the template (or where stuff goes) of the resume is the same, but the content (or the stuff) changes for each job.\n\n\nA resume is different from a CV.  A CV is the list of all your past achievements and deliverables. It is not tailored for a specific posting. The template and the content remain stable until achieving new deliverables such as a presentation, a publication, a new skill etc.\n\n\nThere are thousands of resume templates on the internet that can help you to get the position you desire; the challenge is finding the best resume template for your needs. Remember, the resume and the cover letter introduces you to potential employers and recruiters. But before you even talk with another human, an algorithm will approve your resume for human eyes to look at so make sure to use those keywords that are in the job announcement. Without those, you are unlikely to get an interview.\n\n\nYour resume should be 1) easy to read 2) easy to find keywords related to the announcement 3) be honest about your skills and abilities and 4) represent who you are as a potential colleague\n\n\nThis is for computer science students but it gives some nice examples of how to make this work for recent grads without much experience (keep scrolling on it, so many ads https://zety.com/blog/computer-science-resume). Also CrashCourse has a good series on Youtube (https://www.youtube.com/watch?v=T_4JBbeGsSE). The example provided is a bit more CV like than resume like. See if you can improve on it. \n\n\nInstructions: Write a 1 page resume for an ecological job with data science skills/tools you think you would like. Highlight what skills you need but don’t have for this type of work using a different color text. Resume should be no longer than a page (eek, we know).\n\n\nFor those of you looking for extra challenges in this exercise\n\n\nProfessional Challenge: Create this assignment with data science features (e.g. link to your GitHub, data science knowledge skills and abilities you have and/or hope to have by the end of class)\n\n\nCoding Challenge: Create this assignment in markdown"
  },
  {
    "objectID": "occ_map.html",
    "href": "occ_map.html",
    "title": "",
    "section": "",
    "text": "Occurrence Map\n\nIn this lesson, you will learn\n\nHow to plot species occurrence data onto a world map\n\n\n\nInstall and load packages\n\ninstall.packages(\"tidyverse\", repos = \"http://cran.us.r-project.org\") #includes ggplot\n\nInstalling package into '/Users/MilaUserAccount/Library/R/x86_64/4.2/library'\n(as 'lib' is unspecified)\n\n\n\nThe downloaded binary packages are in\n    /var/folders/8_/66ctd00n6b9839b2ghhk1gjw0000gp/T//RtmpMaKige/downloaded_packages\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the \u001b]8;;http://conflicted.r-lib.org/\u0007conflicted package\u001b]8;;\u0007 to force all conflicts to become errors\n\n\n\n\nRead the data\n\nranaData <- read_csv(\"ranaData.csv\")\n\nNew names:\nRows: 1778 Columns: 4\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" dbl\n(3): ...1, longitude, latitude date (1): eventDate\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...1`\n\n\n\n\nSetting the x and y limits of our map\nWe want our map to be centered on where our occurrence data is found. To do this, we will create x and y limits based on the latitude (y) and longitude (x) values of our cleaned data.\n\nxmax <- max(ranaData$longitude) \nxmin <- min(ranaData$longitude) \nymax <- max(ranaData$latitude) \nymin <- min(ranaData$latitude)\n\n\n\nFinding the date range\nSince we didn’t specify the date range in our data acquisition call from GBIF, our species occurence data spans an unknown date range. It’s a good idea to include the date range in the title of the plot. We can use the range function.\n\nrange(na.omit(as.Date(ranaData$eventDate)))\n\n[1] \"1891-09-30\" \"2023-02-20\"\n\n# our date identified ranges from 1891 to 2023\n\n\n\nMapping\nWe will use ggplot2 to plot our data and style our map.\n\nwrld <- ggplot2::map_data(\"world\") # obtaining world borders\n\nggplot() + # call ggplot\ngeom_polygon(data = wrld, mapping = aes(x=long, y=lat, group=group), fill=\"grey75\", colour = \"grey60\") + # add world shapes, which are polygons stored in the df wrld\n                   # we can specify the color, which is the border, and the fill, which is                      the inside of the shape\n  geom_point(data = ranaData, mapping=aes(x=longitude, y=latitude), show.legend=FALSE) + # our lat/long data are points on an x/y grid. We suppress the legend. \n  labs(title=\"Species occurrences of R. draytonii from 1891 - 2023\", x=\"longitude\", y=\"latitude\") + # add labels to the graph\n  coord_fixed(xlim = c(xmin, xmax), ylim = c(ymin, ymax)) + # give it our x and y limits to center the graph\n  scale_size_area() + # scales the map accordingly \n  borders(\"state\") # add state borders"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Info",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "",
    "section": "",
    "text": "Course Overview\n\nDescription\nWelcome to Applied Data Science. This class is a structured practicum with the goal of supporting students to build knowledge and skills relevant to applying data science to issues of species conservation. The class is a collaboration between students at the University of Arizona and Lewis & Clark College. We will work together with professionals at the US Fish & Wildlife Service who are on the front lines of managing endangered species. You will work in teams to organize, clean, analyze and visualize data to help improve conservation approaches and management efforts for species of concern in the US. In this process you will be introduced to the realities of conservation jobs in governmental and nonprofit organizations and help you build professional skills that are relevant in these fields. You will also learn more about data science in general and project management best practices of working as a team.\n\n\nCourse Prerequisites\nNone, but students should have had beginning experience with R programming language demonstrated in course work or other activity.\n\n\nLearning Objectives\nDuring this course, we expect students to:\n\nApply computational methods for handling and using data to inform questions and solve problems;\nImplement professional practices to produce data-informed deliverables useful for stakeholder decision making;\nPractice communicating insight from data through written, visual, and verbal media to a variety of audiences;\nCreate a professional toolbox of applied data science skills useful for applications of interest and future careers;\nSummarize and appraise data science approaches as they relate to addressing real-world challenges.\n\n\n\nLearning Outcomes\nBy the end of this course, students will:\n\nDefine, differentiate, and explain the nature and application of modern computational methods for handling and using data as it relates to real-world situations and challenges;\n\n-Associate, examine, and compare how to infer meaning and insight from data through written, visual, and verbal communication to both experts and non-experts;\n\nSummarize, implement, and appraise data analytics and their interpretation in relation to real-world circumstances;\n\n-Practice, manage, and design a personal toolbox of data skills useful for various careers and professions"
  },
  {
    "objectID": "sdm_current.html",
    "href": "sdm_current.html",
    "title": "",
    "section": "",
    "text": "What an SDM is\nHow to create an SDM with the machine learning method, MaxEnt\nHow to predict your SDM result onto a geographic area given current environmental variables\nHow to predict your SDM result onto a geographic area given future environmental variables under CMIP5\n\n\n\n\nA Species Distribution Model. More here?"
  },
  {
    "objectID": "HW_6.html",
    "href": "HW_6.html",
    "title": "",
    "section": "",
    "text": "HW 6: Current & Future SDM\nSpecies Distribution Modeling Instructions\n\n\nUsing the code from class, create \n\nA current species distribution of your focal SSA species\nA future species distribution (70 years from now) of your focal SSA species \n\n\n\nEach data visualization of the model needs to include\n\nA descriptive title\nAxes labels\nClear legend \nCountry and state borders\n\n\n\nEach person needs to turn in\n\nAn image of your current species distribution of your focal species\nAn image of your future species distribution (70 years from now) of your focal species\nThe .R file you used to create those images\n\n\n\n\nFor those of you looking for extra challenges in this exercise\n\n\nProfessional Challenge: Add a paragraph explaining this figure similar to what you would see in an SSA publication. E.g. Figure 2: Maxent Species Distribution Model of the Foothill Yellow-Legged Frog (Rana boylii). This species is mostly detected on coastal California however suitable habitat extends into the central valley. Occurrence data was downloaded from GBIF on 02/03/2023 (doi:) and available on GitHub (link).  \n\n\nCoding Challenge: Transfer your .R  to a .md with instructions ready to be published on GitPages for others to learn how you did your model."
  },
  {
    "objectID": "HW_4.html",
    "href": "HW_4.html",
    "title": "",
    "section": "",
    "text": "HW 4: Occurrence Map\n2023 Occurrence Map Instructions\n\n\nUsing the species your group decided on, each person needs to turn in a .R file in which you\n\nObtain data\n\nFrom  GBIF\nSet limit = 4000\n\nClean data \n\nRemove duplicates\nRemove where lat/long are NA\nRemove points where occurrence status is not present\nplaces that are not logical… be sure to justify with comments what points you determined were not logical ie. this species is endemic to Oregon so I can remove points in Australia\n\nCreate a map of the occurrence points\n\nAxes labels\nBorders of countries and states\nDescriptive title\n\n\n\n\nUsing comments, write a sentence or two describing what the occurrence points are. \n\n\nYou will have class time on Tuesday Feb 14 to work on this."
  },
  {
    "objectID": "HW_5.html",
    "href": "HW_5.html",
    "title": "",
    "section": "",
    "text": "HW 5: Natural History and Taxonomy\nNatural History Information Instructions\n\n\nFill in the following subsections for your group’s chosen species. Each person will submit their own work. Later when making the final document for USFWS, you will aggregate and edit everyone’s information into one document. This homework should be a minimum of 1 page single spaced. (Note - interpret “pp” below as paragraphs, not pages for your answers to this.)\n\n\nSee SSA for CactusFerruginousPygmyOwl https://ecos.fws.gov/ServCat/DownloadFile/211651 for what this looks like in the final report. We are aiming for your sections to be included in the document, but not provide all of the content (USFWS needs to make a lot of content!). \n\n\nTaxonomy: (1-2 pp) What is this species referred to in the literature and how confident are we that it isn’t a different species? E.g. species name, taxonomic affiliations, systematic relationships (ie close relatives), species status (ie how certain are we about the taxonomy)\n\n\nKey Identification Characteristics: (1 pp) what characteristics do you use to id this species\n\n\nHabitats: (1-2 pp) What kinds of habitats does your species affiliate with (ie riparian, alpine, thorn scrub, etc)\n\n\nHistoric Range: (1 pp) Where is the species thought to have occurred in the last 50 years 9e.g. Western North America, Central North America, Northern Florida etc, etc)\n\n\nReferences: Please reference your information as best you can including primary sources whenever possible. Again this makes it easier for USFWS to make their final SSA report.\n\n\nFor those of you looking for extra challenges in this exercise\n\n\nProfessional Challenge: Add another section on key life history and/or population movement components you think will be useful for determining status of this species under the 3 Rs (Resiliency, Representation, and Redundancy)\n\n\nCoding Challenge:Make your informational interview summary as a markdown file ready to be published on GitPages\n\n\n\nPp = paragraph"
  },
  {
    "objectID": "HW_1.html",
    "href": "HW_1.html",
    "title": "",
    "section": "",
    "text": "HW 1: Data Science Tools\nHW 1: Data Science Tools Instructions\nPurpose: The goal of this homework is to introduce you to the data science tools we will use in this class. We do not expect you to come into this class with this knowledge. Some of these tools will likely be new to everyone.\nBackground: Data Science is an ever changing assortment of tools and approaches. In this course, we will use a variety of them to create a report on a species that is being evaluated for endangered species status. You will learn the natural history of a species and use data to create maps related to where the species has been found and where it could be found given the same environmental conditions using models. You will then share your report with the world on the internet using some of the tools listed below.  \n\nThis homework will introduce you to/ remind you of the following programs that are commonly used in ecological research to create projects like the one you will do this semester. All of these are essential tools in ecological and conservation research, for now. \n\nR Language for Programming\nR packages\nR Studio\nGit\nGitHub\nR Markdown\nZenodo \n\nTo Do: In a couple paragraphs for each tool (~2-4 page single-spaced total), explain what each of the above tools is, and what it is primarily used for in data science. Of course you may Google these, but describe them in your own words. For each of these tools write a sentence or two about how they might be used in a project applied to conservation, like the one you will create this semester.\n\n\nFeel free to use headers and subheaders to organize your work. For example, \nR Programming Language (https://www.r-project.org)\nWhat is R? \nR is an open source language and environment for statistical computing and graphics derived from the S language and environment developed in Bell Laboratories (AT&T) to turn ideas into software, quickly, and faithfully. Etc, etc, etc\n\nWhy use R for statistical computing and graphics?\nR provides a transparent and reproducible way to analyze and visualize data \n\nHow and why will I use R in my research project / life ?\nFor everything! All the R. I use it to calculate my taxes and weekly budget\n\n\nFinally, please create a GitHub account.\n\n\nFor those of you looking for extra challenges in this exercise\n\n\nProfessional Challenge: Identify 2 new skills from this exercise that you would now like in your course personal goals\n\n\nCoding Challenge: Create this assignment in markdown \n\n\nTo Complete this Homework:\n\nL&C students upload your document in Moodle\nUA students upload your document into D2L\nUpdate the google sheet below with your GitHub and Slack Username https://docs.google.com/spreadsheets/d/1ooLI9HfDxCiuGlLvv8L5sxLxbeWj6d5IGIapKAPUdA4/edit?usp=sharing\nIf you have not joined Slack (in class activity), please do so here."
  },
  {
    "objectID": "for_students.html",
    "href": "for_students.html",
    "title": "Class Schedule",
    "section": "",
    "text": "Week\nDate\nTopics during class time\nAssignments due\nImportant Notes\n\n\n1\nJan 10\nClasses start UAZ, no class this week\nNA\nUAZ starts on Wed\n\n\n2\nJan 17\nIntroductions, surveys, creating github account, and preparing for stakeholder meeting\nIn class surveys, In class GitHub registration\nFirst day of class for both schools, grant evaluation survey, getting to know you survey\n\n\n3\nJan 24\nEndangered species act lecture, meet USFWS collaborators, and stakeholder interviews\nHW 1 - Data science tools homework\nPreclass reading on USFWS and ESA; Stakeholder visit with time for informational interviews. HW 1 due.\n\n\n4\nJan 31\nGroup assignments, R studio, github, and project management\nHW 2 - One page resume, summary from informational interviews, Personal course goals\nAssign teams, Preclass reading on consulting/project management basics. HW 2 due.\n\n\n5\nFeb 7\nData acquisition, cleaning, and visualization using GBIF\nHW 3 - GitHub Repo with ReadMe for each group\nPreclass reading on coding good enough practices and project management in github. HW 3 due.\n\n\n6\nFeb 14\nProject group work. Focus species data acquisition, cleaning, and mapping\nHW 4 - Occurrence map\nPreclass reading on occurrence data wrangling. HW 4 due.\n\n\n7\nFeb 21\nSpecies distribution models (SDMs)\nHW 5 - Natural history/taxonomy summary\nPreclass reading on SDMs. HW 5 due.\n\n\n8\nFeb 28\nFuture SDM in class lesson. Time to discuss deliverables and work flow as a team (time to start updating repos). In class work time for your focal species current and future SDM.\nHW 6 - SDMs\nHW 6 due Friday at midnight.\n\n\n9\nMar 7\nLC meets only. Focus species project group work on organizing GH repo and materials for sharing with stakeholders.\n\n\nUA Spring Break\n\n\n10\nMar 14\nProject group work on presentation and GH repo to share with stakeholders.\n\n\nPreclass reading on giving and receiving feedback\n\n\n11\nMar 21\nReview group progress. Give a short presentation in class on your species. Each stakeholder will be assigned to 1-2 groups and evaluate deliverables (e.g. taxonomy review, occurrence maps, SDMs) and GitHub Repo\nPresentation to stakeholders (GH repo and info on your sp)\n2nd USFWS stakeholder visit\n\n\n12\nMar 28\nUA meets only, Project group work re stakeholder feedback\n\n\nLC Spring Break\n\n\n13\nApr 4\nGroup work re stakeholder feedback\n\n\n\n\n\n\n14\nApr 11\nGroup work for final presentations\n\n\n\n\n\n\n15\nApr 18\nStakeholder visit: showcasing student deliverables. Listen and evaluate student presentation and GitHub Repo\nFinal presentation to stakeholders\nFinal stakeholder visit\n\n\n16\nApr 25\nSubmit final project. Reflection and evaluation.\nFinal project submission on github, in-class reflection and survey\nLast day of class for both institutions, recap and survey in class. Good luck with finals!"
  },
  {
    "objectID": "for_students.html#reading-schedule",
    "href": "for_students.html#reading-schedule",
    "title": "Class Schedule",
    "section": "Reading Schedule",
    "text": "Reading Schedule\n\n\n\nRead by class on\nLinks\nReading Questions\n\n\nJan 24\nWhat is a Species Status Assessment (SSA)?\nThe Endangered Species Act (ESA)\nEcological Services provided by the US Fish and Wildlife Service\nWhat US Fish and Wildlife Service does with the ESA\n\n\n\nJan 31\nR (programming language)\nIDE\nThe Center for Open Science\nUNESCO Recommendations for Open Science\n\n\n\nFeb 7\nData cleaning\niNaturalist and Scientific Discoveries - read through Data Validation\nUses of GBIF in conservation  - watch the video\n\n\n\nFeb 14\nScientific article on public database accuracy for SDMs - case study, read introduction and discussion\nAbstract from Biodiversity Information Science and Standards 2022 - conference proceedings abstract\nUSGS Wildlife wrangler GitLab - read ReadMe.md especially Valuable Features\n\n\n\nFeb 21\nEcological niche\nGeneral intro to the concept of SDM\nMore detailed slide deck about SDM\nAbout MaxEnt, a type of SDM\n\n\n\nFeb 28\n\n\n\n\n\nMarch 7\n\n\n\n\n\nMarch 14\n\n\n\n\n\nMarch 21\n\n\n\n\n\nMarch 28\n\n\n\n\n\nApril 4\n\n\n\n\n\nApril 11\n\n\n\n\n\nApril 18"
  },
  {
    "objectID": "coding_basics_for_this_course.html",
    "href": "coding_basics_for_this_course.html",
    "title": "",
    "section": "",
    "text": "While this course doesn’t have any specific prerequisites, it is important to have a basic understanding of coding in R. Before attempting the lessons, you should be familiar with all of the content in this Coding Basics tab. There are three small lessons intended to get you familiar with the skills you will need to succeed in the remainder of the course.\n\n\n\nThere are three lessons, and three practices to gain more experience with what you will learn in the lessons. Each lesson is in its own file, but all of the practices are in one file.\n\n\n\n\n\n\nGain some familiarity with rstudio\nLearn how to run R code in “code chunks”\nPerform some arithmetic with R\nCreate variables in R, and assign values to them\nFunctions\n\n\n\n\n\nInstalling and loading a package\nOpening and looking at a data set\nAccessing a specific column in a data set\nSelecting columns to create a new variable\nRelational operators\nFiltering\nThe pipe %>%\n\n\n\n\n\nLoad and filter data sets as needed\nLearn the basic elements of ggplot"
  },
  {
    "objectID": "HW_3.html",
    "href": "HW_3.html",
    "title": "",
    "section": "",
    "text": "HW 3: Practice with GitHub\nNote: It is critical you do these steps in the exact order outlined below.\n\n\nGetting your group repository onto your posit cloud\n\n\n\nLog in to your personal GitHub account\nNavigate to our class’ organization, BiodiversityDataScienceCorp\nFind the repository that has your group name on it\nClick the green code button and copy the link there\nLog in to your posit cloud account\nCreate a new project: new project from Git Repository\nPaste the link you copied from GitHub when prompted\n\n\n\nYou now should have a project in posit.cloud that is your local copy of your group’s repository. This project should already be populated with a gitignore, license, and README file. There should be a Git tab in the top right corner of R studio.\n\n\nSetting up your group repository for commits/pushes/pulls\n\nIn this new project in rstudio cloud, add a line in the gitignore file that says “github.R”\nCreate a new .R file and name it “github.R”\nCopy the contents from the github.R file you have in the project you created during class\nPaste them to the new github.R file in your group repository. Save it. Highlight all of the code and run it. \n\n\n\nYou now should be able to commit/push/pull to/from GitHub to work collaboratively with your group. \n\n\nTo work collaboratively with GitHub / R studio, follow this flow\n*Only one person edits the same file at a time. If two people edit the same document simultaneously, you will have to do extra work to ensure all of your saves are committed properly. \n*Multiple people can work at once on different files. For example, Person A can edit your README while Person B edits your gitignore. \n\nPerson A: Write some code in a file and click save when you are done\nPerson A: Commit and push the code. Leave a detailed commit message for your future self if you ever need to go back to a previous version. Be sure to push. \nPerson B: Pull first!!! \nPerson B: Edit the file. Save. Commit. Push. \nPerson A: Pull. Etc. You now have a rhythm for working on the same file. \n\n\n\nSetting up your repository’s README file\n\n\nREADMEs are written in markdown. See this markdown cheat sheet for a guide on how to create headings, change the font, etc. \n\n\nYour README file needs to contain, at the least: \n\nYour group members’ names\nYour species\nHave each group member write one paragraph about their favorite species (and commit/push/pull themselves). Remember to take turns. \n\n\n\nTips\n\nAlways save, commit, and push after working on a file\nAlways pull before working on a file\nGive your commit messages descriptions that would help you if you needed to “undo” to go back to a previous version\nCommunicate with your group when you plan to work on a file so two people are not working on the same file at once\n\n\n\n\nFor those of you looking for extra challenges in this exercise\n\n\nProfessional Challenge:Add your github information to your resume\n\n\nCoding Challenge:  Add a photo of your species to your README file. You’ll need to upload the photo to your repo, then you can use markdown syntax to call the photo."
  },
  {
    "objectID": "sdm_future.html",
    "href": "sdm_future.html",
    "title": "",
    "section": "",
    "text": "future SDM\nusing CMIP5\nInstall packages & load libraries IFF they aren’t yet part of your project environment\nThey may have already been loaded in sdm_maxent_lesson.R.\npackages\ninstall.packages(“dismo”) install.packages(“maptools”) install.packages(“tidyverse”) install.packages(“rJava”) install.packages(“maps”)\nlibrary(dismo) library(maptools) library(tidyverse) library(rJava) library(maps)"
  },
  {
    "objectID": "reading_schedule.html",
    "href": "reading_schedule.html",
    "title": "",
    "section": "",
    "text": "Read by class on\nLinks\nReading Questions\n\n\nJan 24\nWhat is a Species Status Assessment (SSA)?\nThe Endangered Species Act (ESA)\nEcological Services provided by the US Fish and Wildlife Service\nWhat US Fish and Wildlife Service does with the ESA\n\n\n\nJan 31\nR (programming language)\nIDE\nThe Center for Open Science\nUNESCO Recommendations for Open Science\n\n\n\nFeb 7\nData cleaning\niNaturalist and Scientific Discoveries - read through Data Validation\nUses of GBIF in conservation  - watch the video\n\n\n\nFeb 14\nScientific article on public database accuracy for SDMs - case study, read introduction and discussion\nAbstract from Biodiversity Information Science and Standards 2022 - conference proceedings abstract\nUSGS Wildlife wrangler GitLab - read ReadMe.md especially Valuable Features\n\n\n\nFeb 21\nEcological niche\nGeneral intro to the concept of SDM\nMore detailed slide deck about SDM\nAbout MaxEnt, a type of SDM\n\n\n\nFeb 28\n\n\n\n\n\nMarch 7\n\n\n\n\n\nMarch 14\n\n\n\n\n\nMarch 21\n\n\n\n\n\nMarch 28\n\n\n\n\n\nApril 4\n\n\n\n\n\nApril 11\n\n\n\n\n\nApril 18"
  },
  {
    "objectID": "bootcamp_practice_1.html",
    "href": "bootcamp_practice_1.html",
    "title": "",
    "section": "",
    "text": "Variables\nFunctions\nPrinting to the screen\n\n\nAssign the number 500 to the variable numberButterflies\n\n\n#\n\n\nCalculate the cubed root of 26 and assign it to the variable y\n\n\n#\n\n\nPrint the variable y\n\n\n#\n\n\n\n\n\n\n\n\nPackages\nAccessing specific columns in a data set\nAssigning variables\nSelect\nFiltering\n\n\n(Install if you haven’t completed lesson 2) and load the tidyverse and palmerpenguins packages.\n\n\n#\n\n\nAssign the bill length column to a variable called penguinBillLength\n\n\n#\n\n\nAssign the bill depth column to a variable called penguinBillDepth.\n\n\n#\n\nFor Questions 4 - 7, you can choose to either use the pipe %>% or not. Either way is great!\n\nAssign only the columns island, bill_length_mm, and bill_depth_mm to a new variable (name it whatever you’d like!).\n\n\n#\n\n\nAssign only the columns sex and year to the variable sexByYear.\n\n\n#\n\n\nCreate a subset of the data that includes all the columns only for male Gentoo penguins with flipper lengths greater than 20 mm.\n\n\n#\n\n\nCreate a subset of the data that includes all penguins on Biscoe island with flipper lengths greater than 20 mm. Remove all of the NA values as well.\n\n\n#\n\nChallenge: Using the pipe %>%, string together two commands with only one line of code to create a data set that includes only the columns: species, sex, and flipper length, with only Adelie penguins whose flippers are greater than 180 mm.\n\n#\n\n\n\n\n\n\n\n\nPackages\nFiltering\nGgplot\n\n\n\n\nEach plot should have axis labels, a title, and use colors to illustrate the data.\n\n(Install if you haven’t completed lessons 2 or 3 or practice 2) and load the tidyverse and palmerpenguins packages.\n\n\n#\n\n\nCreate a subset of the data that includes all the data from the years 2007 and 2008.\n\n\n#\n\n\nCreate a box plot of the bill lengths by male and female penguins of all species from the years 2007 and 2008.\n\n\n#\n\n\nCreate a subset of the data that includes only the female penguins.\n\n\n#\n\n\nCreate a box plot of the bill depth by species of only the female penguins.\n\n\n#\n\n\nCreate a scatter plot of the relationship between bill length and bill depth for penguins on only Dream island and color by species.\n\n\n#"
  },
  {
    "objectID": "bootcamp_3.html",
    "href": "bootcamp_3.html",
    "title": "",
    "section": "",
    "text": "Bootcamp Lesson 3\n\nLearning objectives\n\nLoad and filter datasets as needed\nLearn the basic elements of ggplot\n\n\n\nWhat is ggplot?\nggplot is an R package created by Hadley Wickham (he’s kind of a big deal in the R world). The “gg” translates to “grammar of graphics”, and is founded in the idea that all data visualizations are comprised of three components:\n\ndata set\naesthetics, or visual marks that represent the data (i.e. the stuff that you see)\ngeometric objects, or “geoms” (e.g the type of plot)\n\nggplot is part of the “tidyverse” - a series of packages that share common strategies for working with data and generating visualizations. We can load all the packages at once (including readr, dplyr, and other packages for working with data) by running this command:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the \u001b]8;;http://conflicted.r-lib.org/\u0007conflicted package\u001b]8;;\u0007 to force all conflicts to become errors\n\n\n\n\nGetting Help\nggplot has many, many options and possibilities. When getting started (and even once you’re a ggplot expert), it’s a good idea to refer to documentation. In the “Files” section, click on the “ggplot2-cheatsheet.pdf”. The RStudio “help” function is also useful:\n\nhelp(ggplot)\n\n\nhelp(geom_jitter)\n\nAs always, entering your question or error message into your favorite search engine is trypically a pretty effective way to get help. Chances are good that someone else has already encountered a ggplot problem you’ve encountered, and the answers are likely online.\n\n\nPenguin Data\nWe are going to use the penguins data set again. To call up this data, we need to load the palmerpenguins package.\n\nlibrary(palmerpenguins)\n\nOur goal is to create a box plot that shows the difference in mass between the male penguins of the different species. With any programming problem, it’s good to think about it in terms of steps:\n\nload the data set\nfilter the data so it only contains data I’m interested in\ncreate a chart with that data\n\nOnce you break the problem down into steps, hopefully it’ll be easier to tackle. Plus, each individual step is ‘googleable’, so help is not far away.\nLet’s get the subset of the data that is only the male penguins. Assign this to the variable malePenguins. (Hint: if you don’t know how to do this, look back at the filtering section of lesson_2.Rmd)\n\n# malePenguins <- penguins %>% filter(sex == \"male\")\n\nOur “mary” variable now just has the count of “Marys” over time. Let’s do a line plot to see the change over time. Here is the code we will run:\nggplot(data=malePenguins, mapping=aes(x=species, y=body_mass_g))+geom_box()\nLooking at the code, here’s how we’re including the three basic parts of a ggplot chart:\n\nDATA - data=malePenguins\nAESTHETICS - mapping=aes(x=species, y=body_mass_g)\n\nGEOM - +geom_boxplot() to create a box plot\n\n\n# ggplot(data=malePenguins, mapping=aes(x=species, y=body_mass_g))+geom_boxplot()\n\nYOUR TURN:\nLet’s say you want to see if there is a relationship between a penguin’s body mass and it’s flipper length. Create this scatter plot. You don’t need to filter this data, since we are looking at all of the penguins. The x axis should be the body mass and the y axis should be the flipper length. Look at the ggplot2-cheatsheet.pdf to find out what the GEOM should be.\n\n#\n\n\n\nLabels & Colors\nYou can specify labels to add to your plot by using the “labs()” function:\n\n#ggplot(data=malePenguins, mapping=aes(x=species, y=body_mass_g)) + \n  #geom_boxplot() +\n  #labs(x = \"Penguin Species\", y = \"Body Mass (g)\", title = \"Penguin body mass is different by species\")\n\nAdditionally you can add colors. You can add colors by a categorical variable (like species) by adding it to the aes argument. Here we will use “fill” because we want the color to fill the whole box. If we just have points, we can use “color =” instead.\n\n#ggplot(data=malePenguins, mapping=aes(x=species, y=body_mass_g, fill = species)) + \n  #geom_boxplot() +\n  #labs(x = \"Penguin Species\", y = \"Body Mass (g)\", title = \"Penguin body mass is different by species\")\n\nYOUR TURN: Can you add some labels and color by species to your scatter plot?\n\n#\n\n\n\nSaving your plot\nTo save your plot as a file, you can use the ggsave() function. ggsave will save the last plot generated:\n\n#ggsave(\"plot.png\", height=5, width=5, units=\"cm\", scale=3)\n\nFrom there, you can download your pot file, and use it however you choose.\nAwesome job!\nWe are going to use ggplot to create maps later this semester. The important thing is that you are familiar with the syntax in ggplot, not the specifics of a boxplot or scatter plot."
  },
  {
    "objectID": "bootcamp_2.html",
    "href": "bootcamp_2.html",
    "title": "",
    "section": "",
    "text": "Bootcamp Lesson 2\n\nLearning objectives\n\nInstalling and loading a package\nOpening and looking at a data set\nAccessing a specific column in a data set\nSelecting columns to create a new variable\nRelational operators\nFiltering\nThe pipe %>%\n\n\n\nPackages\nWe often use packages when coding in R. Packages, very simply, are tool boxes that contain useful functions. We will use a package called the “tidyverse.” Tidyverse is designed for data science and is very useful for organizing, manipulating, and viewing our data.\nFirst we have to install a package if it has never been installed in a local project. You only have to do this once per file.\n\ninstall.packages(\"tidyverse\", repos = \"http://cran.us.r-project.org\")\n\nInstalling package into '/Users/ameliapruiett/Library/R/x86_64/4.2/library'\n(as 'lib' is unspecified)\n\n\n\nThe downloaded binary packages are in\n    /var/folders/jw/09h6w34n55534tfy1wjbhrwr7h160v/T//RtmpsEq5u9/downloaded_packages\n\n\nOnce you have installed a package, you need to load it into your workspace. You need to do this every time you run the file.\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the \u001b]8;;http://conflicted.r-lib.org/\u0007conflicted package\u001b]8;;\u0007 to force all conflicts to become errors\n\n\nYour turn. For this lesson, we will use the “palmerpenguins” package, which contains a data set. This data has various measurements of penguins collectd near Palmer Station in Antarctica and was made available by Dr. Kristen Gorman.\nUsing the same syntax as above, install and load the “palmerpenguins” package.\n\n\n\nNow we have access to a data set, called penguins.\n\n\nLooking at a data set and accessing a column\nYou can get a glance of a data set with the function “glimpse.”\n\n# glimpse(penguins)\n\nFrom glimpse above, we can see that the data set contains 8 columns. To access one column, we use the dataSet$column syntax. We can then assign it to a variable.\n\n# islands <- penguins$island\n\nNow your turn. Create a variable (remember, to assign a variable use the syntax: x <- thing) called penguinFlipperLength that is the flipper_length_mm column.\n\n# \n\n\n\nSelecting\nWhile the syntax data$ColumnName is useful in many cases, there are times when we want to have multiple columns in a new variable. For example, we may be interested in only the penguin island and mass. We can do this with the select function.\n\n# penguinIslandMass <- select(penguins, island, body_mass_g)\n\nYour turn. Select species and flipper_length_mm and assign it to the variable penguinSpFlipper.\n\n# \n\nSelect is very useful when we have a large data set that contains information that we are uninterested in, or when we need to put our data in a specific format.\n\n\nFiltering\nLet’s say we wanted to look at just a subset of the penguins. There are a few species of penguins in the data. We could scroll through all 344 rows of the data and note the species, or we could have R do this for us.\nTo know all of the species present in the data, we can use the function ‘unique’. Unique looks for all of the unique values. The argument we will give unique is the specific column that we want to know all of the species of.\n\n# unique(penguins$species)\n\nGreat! Now we see there are three species of penguins. Let’s say we want all of the data for only the Adelie penguins. We can accomplish this with filtering. The syntax is filter(data, column name relational operator value). Note that we need to put Adelie in quotes because it is a word. If we filter by a number, we don’t need quotes.\n\n# adelieData <- filter(penguins, species == \"Adelie\")\n\n\n\nRelational operators\nIn the filter example above, we used “==” as part of our condition argument. The double equals is an example of a relational operator - it’s a character (or multiple characters) that represents a logical action or process. Practically speaking, the double equals means “is this field equal to this value?”. If the answer is “TRUE”, then the row is included as part of the filtered data set.\nHere are some other relational operators:\n\n> (greater than)\n< (less than)\n<= (less than or equal to)\n>= (greater than or equal to)\n!= (not equal to)\n\nIn the filter function, relational operators are used to define a condition.\nYour turn. Create a variable called longFlipperData that is the subset of the penguin data with flipper length in mm greater than 22.\n\n#\n\n\n\nLogical Operators\nThere may be cases in which we want to filter our dataset based on more than one condition. In these cases, we would use logical operators. Maybe we want to find all the penguins that are both female and weigh over 5000 g. Here are the main logical operators:\n\n& (and)\n| (or)\n\nIn the filter function, logical operators are used to join conditions together.\nHere’s an example of how to use a logical operator with the filter function:\n\n# femaleHeavyPenguins <- filter(penguins, sex == \"female\" & body_mass_g > 5000)\n\nYour turn. Create a variable called biscoeAndDreamSmall that contains all the penguins from either Biscoe OR Dream Island that are less than 3000 g.\n\n#\n\n\n\nThe “Pipe”\nThe Tidyverse introduced a new convention to R called the “pipe”:\n%>%\nThe purpose of the pipe is to string functions and data together. You can think of it as sort of the glue that joins pieces of an assembly line together. Another way to think of it is to read it as “AND THEN”.\nBelow we can rewrite a command using the filter function with the pipe. After the assignment symbol (<-) we start with the data set, followed by the pipe, followed by the filter function. What’s different about the arguments in the filter function in this case?\n\n#adelieData2 <-penguins %>% filter(species == \"Adelie\")\n#adelieData2\n\nTry using the pipe in the code chunk below to create a data subset of penguins that are male and assign it to the variable malePenguins. Print it to the screen.\n\n# create and print malePenguins. \n\nYou can also use the pipe for select (and many other functions as well). Instead of putting the data name inside of the function, you can put it before the pipe.\n\n#penguinIslandMass2 <- penguins %>% select(island, body_mass_g)\n\nYour turn. Use the pipe syntax to assign the columns bill_length_mm and sex to a new variable with select.\n\n#\n\n\n\nRemoving NA values\nOur penguin data set contains NA values in some rows. Often, NA values create problems for calculating statistics and running models. There are many ways of doing this. One is to remove all rows with NAs in any column using the function na.omit.\n\n#noNAPenguins <- na.omit(penguins)\n\nYou can see that there are 333 rows (by the 333 observations in the Environment tab). We started with 344 rows in the original penguin set.\nGreat job!"
  },
  {
    "objectID": "bootcamp_1.html",
    "href": "bootcamp_1.html",
    "title": "",
    "section": "",
    "text": "Let’s start off by clicking the “Visual” button above, so this document is a little easier to read and work with. You’ll likely get a popup window the first time, confirming that you want to switch to Visual Mode. Just click Use Visual Mode\n\n\n\nGain some familiarity with rstudio\nLearn how to run R code in “code chunks”\nPerform some arithmetic with R\nCreate variables in R, and assign values to them\nFunctions\n\n\n\n\nYou’ll notice rstudio is divided into 4 windows. Let’s take a look at each one: - upper-left: this is the area for reading / writing files. We’ll probably use this window the most this semester. - lower-left: this is the console, where you can run R code directly. You might use this occasionally, but for the most part probably won’t have to. - upper-right: this window has a couple useful tabs: - Environment: any variables you create will be listed here. This can be useful for quick reference. - History: this shows the R commands you’ve run within a session. - Connections: this will be important for using GitHub, which we will get to later in the semester - lower-right: this window lists any files and folders that are part of your project/assignment. It also has tabs that can display plots and help documentation. You can resize windows / borders as needed in order to more easily focus on the particular window you’re using.\n\n\n\nThe grey rectangles below are “code chunks”, where you can type and execute R code. We will use them in these lessons. In class, you will use .R files, which essentially are entirely code chunks. To run the code, click the green triangle in the upper-right corner of the code chunk. Let’s try running the code chunk below, which is just some simple arithmetic:\n\n2 + 5 \n\n[1] 7\n\n\nNow it’s your turn. In the code chunk below, enter a command to subtract 7 from 18, and run it:\n\n# Enter code below (by the way, I'm a comment. I'm a way for you to leave notes in the code. Simply put a hashtag and type away.)\n\n\n\n\nProgramming languages, including R, rely heavily on the use of variables. A variable is something you define and assign a value to. In the code chunk below, we’re creating a variable called x, and assigning the value of 7 to it:\n\n# Notice we use a 'left pointing arrow' to assign the value\nx <- 7 \n\nWe don’t get any real feedback from running the code above, but it’s often a good idea to check your work by printing a variable’s value to the screen. To do this, you can run code that simply lists the variable, here x. R will remember the variables you assign. You can see what R is “remembering” by looking at the environment tab in the top right.:\n\n# prints to the screen:\nx\n\n[1] 7\n\n\nIt’s worth noting that you can run multiple lines of code within a code chunk:\n\n# create a variable \"penguinCount\" and give it the value 3\npenguinCount <- 3\n# print to screen\npenguinCount\n\n[1] 3\n\n\nNow it’s your turn. Work with your group / people next to you, and do the following:\n\ncreate a variable called y, assign three plus eight to it, and print to the screen.\ncreate a variable z, and assign x+y to it, and print to the screen\n\n\n# create the variable \"y\" here, and print to screen\n\n\n# create the variable \"z\" here, and print to screen\n\n\n\nCoding languages, including R, have functions that help you quickly execute common tasks. Functions typically take the form of:\nfunctionName(argument1, argument2, etc….)\nArguments are the inputs you send to a function, so it has all the information it needs to perform its operation.\nFor example, the function sqrt(number) takes the square root of a number. This lets us quickly compute the answer, rather than having to write the formula for a square root.\n\nsqrt(9)\n\n[1] 3\n\n\nYOUR TURN: In the chunk below, create a variable z, set it equal to the square root of 90, and print it out:\n\n\n\nOne nice thing about rstudio is that you can readily access documentation for functions by using the “help” command:\n\nhelp(sqrt)\n\nThe documentation appears in the lower right window in the “help” tab.\nOne key question: how do you know what functions exist, and what they do?\nAnswer: you Google what you’re trying to do! In the case of R, you might search “How do I do ‘x’ in R?”\nLet’s say you are interested in calculating the absolute value (positive distance from zero) of -35 in R. Take a moment with your group/neighbors, and try to find the answer by searching the internet. In the code chunk below, use the function you found to compute this calculation:\n\n# compute the absolute value of -35\n\nYou made it!"
  },
  {
    "objectID": "sdm_current.html#current-sdm",
    "href": "sdm_current.html#current-sdm",
    "title": "",
    "section": "Current SDM",
    "text": "Current SDM\n\nObtaining and formatting occurence data\nFirst we need to read in our species occurence data. We will make two copies of this data. One copy is the plain data frame that we made before. Another we will convert to spatial points, which is necessary for the model later on.\n\nranaData <- read_csv(\"ranaData.csv\") # read the data\n\nNew names:\nRows: 1778 Columns: 4\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" dbl\n(3): ...1, longitude, latitude date (1): eventDate\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...1`\n\nranaDataNotCoords <- ranaData %>% dplyr::select(longitude, latitude) # copy 1\n\nranaDataSpatialPts <- SpatialPoints(ranaDataNotCoords, proj4string = CRS(\"+proj=longlat\")) # copy 2: spatial points\n\n\n\nObtaining climate data\nWe need to obtain current climate data. We will get the bio variables from world clim at a 2.5 minute resolution. To see what each variable is, see the worldclim website. You can see more details on this function with ?raster::getData\n\ngetData(\"worldclim\", var=\"bio\", res=2.5) \n\nWarning in getData(\"worldclim\", var = \"bio\", res = 2.5): getData will be removed in a future version of raster\n. Please use the geodata package instead\n\n\nclass      : RasterStack \ndimensions : 3600, 8640, 31104000, 19  (nrow, ncol, ncell, nlayers)\nresolution : 0.04166667, 0.04166667  (x, y)\nextent     : -180, 180, -60, 90  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 \nnames      :  bio1,  bio2,  bio3,  bio4,  bio5,  bio6,  bio7,  bio8,  bio9, bio10, bio11, bio12, bio13, bio14, bio15, ... \nmin values :  -278,     9,     8,    64,   -86,  -559,    53,  -278,  -501,  -127,  -506,     0,     0,     0,     0, ... \nmax values :   319,   213,    96, 22704,   489,   258,   725,   376,   365,   382,   289, 10577,  2437,   697,   265, ... \n\n\nThe data we obtained are rasters. A raster consists of a matrix of cells (or pixels) organized into rows and columns, (or a grid) where each cell contains a value representing information, such as temperature. Rasters are digital aerial photographs, imagery from satellites, digital pictures, or even scanned maps.\nTo read more about rasters, check out this article on arcgis.\nNow we will create a list of files in the climate data folder, called wc2-5.\n\ncurrentClimList <- list.files(path = \"wc2-5/\", pattern = \".bil$\", full.names = T)\n\nWe can take this list and create a raster stack, so we can process all the bioclim variables at one time.\n\ncurrentClim <- raster::stack(currentClimList)\n\nWe can check out a single layer of the data we just got. Here i’ll plot bioclim 3, isothermality, and add our species occurrence data on top.\n\nplot(currentClim[[3]]) \nplot(ranaDataSpatialPts, add = T)\n\n\n\n\nLooks good! We have the world climate data, and our points are where we would expect them to be.\n\n\nAdding background points\nMaxEnt requires background points, points where we do NOT have occurrence data. We will walk through the steps required to generate that here.\nFirst we need a raster layer to make the points up on. We can pick any of the currentClim.\n\nmask <- raster(currentClim[[1]]) # mask is the raster object that determines the area where we are generating pts\n\nWe want to generate our random points reasonably nearby our occurrence points. For example, we know this frog species is only found in the US state of California, so it doesn’t make sense to generate absence data on any other continent than North America. If your species is globally spread, or theoretically could be, you will need to change the details in the random points call next.\nWe can find the geographic extent of our data, which requires spatial points data.\n\ngeographicExtent <- extent(x = ranaDataSpatialPts)\n\nWe need to generate at least 1,000 random points. We have 4000 occurrence points, so we will generate as many background points as occurrence points that we have.\n\nset.seed(7536) # seed set so we get the same background points each time we run this code\n\nbackgroundPoints <- randomPoints(mask = mask, # shape we want our points generated onto \n                                 n = nrow(ranaDataNotCoords), # needs to be at least 1000\n                                 ext = geographicExtent, # our geographic extent\n                                 extf = 1.25, # how much bigger than our occ extent do we want background pts\n                                 warn = 0) # don't complain about not having a coordinate reference system\n\nFinally, we rename the columns of our background points from x & y to longitude & latitude so they make sense in light of mapping.\n\ncolnames(backgroundPoints) <- c(\"longitude\", \"latitude\")\n\n\n\nExtract the climate data for the occurrence and background points\nWe need the climate data at each of our occurrence and background points so MaxEnt can figure our which environmental conditions are (or are not) associated with our species.\n\noccEnv <- na.omit(raster::extract(x = currentClim, y = ranaDataNotCoords))\nabsenceEnv <- na.omit(raster::extract(x = currentClim, y = backgroundPoints)) \n\n\n\nCreate data frame with presence training data and backround points (0 = abs, 1 = pres)\nWe need objects that contain all the data necessary to run MaxEnt: the response variable (a vector of 1s and 0s for presences and absences) and the predictor variables (the climate data associated with each occurrence or background point).\nCreate the response variable.\n\npresenceAbsenceV <- c(rep(1, nrow(occEnv)), rep(0, nrow(absenceEnv))) \n\nCreate the predictor variables data frame.\n\npresenceAbsenceEnvDf <- as.data.frame(rbind(occEnv, absenceEnv))\n\n\n\nCreate SDM with MaxEnt\nFinally we will creat our SDM with MaxEnt. We are using the dismo package, which actually runs the MaxEnt in java.\n\nranaSDM <- dismo::maxent(x = presenceAbsenceEnvDf, # env conditions \n                         p = presenceAbsenceV, # p = 1, a = 0 \n                         path = \"maxent_outputs\" )\n\nWe can view the results of the MaxEnt model\n\nresponse(ranaSDM)\n\n\n\n\n\n\nPlot the model: getting the right area\nOur climate data, currentClim, is huge and it isn’t reasonable to predict the distribution of our species over whole world .\nWe will make it smaller. The prediction extent needs to be the same as the extent of our random points generation.\n\npredictExtent <- 1.25 * geographicExtent \n\nNow we can create the geographic area over which we want our model prediction\n\ngeographicArea <- crop(currentClim, predictExtent) \n\nAnd finally, we can predict our SDM over this area.\n\nranaPredictPlot <- raster::predict(ranaSDM, geographicArea)"
  },
  {
    "objectID": "sdm_current.html#future-sdm",
    "href": "sdm_current.html#future-sdm",
    "title": "",
    "section": "Future SDM",
    "text": "Future SDM\n\nObtaining future climate data\nWe will use getData from the raster package. We will work on migrating towards CMIP6, which will require a different data download method in the future. For now, this will work. You can specify which rcp, which is the greenhouse gas emission prediction. You can also specify which climate model you’d like to use, how many years into the future: 50 or 70 years from now.\n\nfutureClim <- raster::getData(name = 'CMIP5', var = 'bio', res = 2.5, rcp = 45, model = 'IP', year = 70)\n\nWarning in raster::getData(name = \"CMIP5\", var = \"bio\", res = 2.5, rcp = 45, : getData will be removed in a future version of raster\n. Please use the geodata package instead\n\n\nThe futureClim data has names that are inconsisent with the currentClim. We can rename them so we can predict the SDM onto this new layer.\n\nnames(futureClim) = names(currentClim) \n\nWe can also look at current vs future climate variables, and see how they change.\n\nplot(currentClim[[1]])\n\n\n\nplot(futureClim[[1]])\n\n\n\n\n\n\nPlot the model: getting the right area\nWe need to crop our future climate data, just like we did with our current climate data. We will use the predictExtent we created in the current SDM.\n\ngeographicAreaFuture <- crop(futureClim, predictExtent)"
  }
]