[
  {
    "objectID": "data_aquisition_cleaning.html",
    "href": "data_aquisition_cleaning.html",
    "title": "",
    "section": "",
    "text": "Querying, cleaning, and mapping GBIF data\n\nIn this lesson, you will learn\n\nWhat species occurrence data is\nHow to obtain publicly-available occurrence data from GBIF\nHow to plot occurrence data with ggplot\nHow to clean occurrence data\n\n\n\nWhat is species occurrence data?\nOccurrence data is a record of where a species was at a given time.\n\n\nHow can we get occurrence data?\nWe can use the Global Biodiversity Information Facility. Data on GBIF is sourced from a wide variety of places: from users on iNaturalist equipped with a smartphone to museum specimens. You will read more about the accuracy of citizen science collected occurrence data on February 14.\nFirst we need to load the necessary packages. We will use the tidyverse for data cleaning and visualization. We will use a package called spocc, which stands for “interface to SPecies OCCurence data sources.”\n\n#Install packages and load libraries:\n\ninstall.packages(\"spocc\", repos = \"http://cran.us.r-project.org\")\n\nInstalling package into '/Users/ameliapruiett/Library/R/x86_64/4.2/library'\n(as 'lib' is unspecified)\n\n\n\n  There is a binary version available but the source version is later:\n      binary source needs_compilation\nspocc  1.2.0  1.2.1             FALSE\n\n\ninstalling the source package 'spocc'\n\ninstall.packages(\"tidyverse\", repos = \"http://cran.us.r-project.org\") #includes ggplot\n\nInstalling package into '/Users/ameliapruiett/Library/R/x86_64/4.2/library'\n(as 'lib' is unspecified)\n\n\n\nThe downloaded binary packages are in\n    /var/folders/jw/09h6w34n55534tfy1wjbhrwr7h160v/T//RtmpN1vdPY/downloaded_packages\n\nlibrary(spocc)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the \u001b]8;;http://conflicted.r-lib.org/\u0007conflicted package\u001b]8;;\u0007 to force all conflicts to become errors\n\n\nRemember to use lots of comments on your own copy of this code, to help yourself remember later on!\nWe can directly query occurrence data in R with the “occ” function, which is part of the spocc package.Take a look at the documentation. In groups: write a query for Rana draytonii from GBIF, from any country during any time period. Limit it to 5000 occurrences.\n\nmyQuery <-occ(query=\"Rana draytonii\", from=\"gbif\", limit=5000)\nmyQuery\n\nSearched: gbif\nOccurrences - Found: 4,632, Returned: 4,632\nSearch type: Scientific\n  gbif: Rana draytonii (4632)\n\n\nGreat! Now we have our query. We need to access the actual data from GBIF. We will use the “$” to navigate the layers of this data.\n\nrana <- myQuery$gbif$data$Rana_draytonii\nView(rana)\n\nLet’s initially plot this data on a map. It can be especially useful with species occurrence data to look at the distribution on a world map to find any outliers. We will use the map data in ggplot2 to add country lines as well.\n\nwrld<-ggplot2::map_data(\"world\") # obtaining world borders\n\nggplot()+\n  geom_polygon(data=wrld, mapping=aes(x=long, y=lat, group=group), fill=\"grey75\", colour=\"grey60\")+\n  geom_point(data=rana, mapping=aes(x=longitude, y=latitude), show.legend=FALSE)+\n  labs(title=\"Species occurrences of R. draytonii\", x=\"longitude\", y=\"latitude\")\n\nWarning: Removed 1773 rows containing missing values (`geom_point()`).\n\n\n\n\n\nNow we want to clean our data. What do we need to do based on the reading for today? We need to: remove outliers (outside of normal sp range), remove duplicates, and deal with NA values. Looking at the map, we can see there are a few points in Africa. This is likely due to a recording error of the location, as they are near (0,0). We can also remove them on the basis of pre-existing knowledge that these frogs are only found in the southwestern United States. We are going to use the pipe %>% to string together a series of commands.\n\ncleanRana <- rana %>% \n  filter(longitude < 0) %>%  # this is one way of removing the pts in Africa\n  filter(latitude != \"NA\", longitude != \"NA\") %>%  # remove places where lat or long are NA\n  mutate(location = paste(latitude, longitude, dateIdentified, sep = \"/\")) %>% # remove duplicates: create a new column that states the location and time\n  distinct(location, .keep_all = TRUE) # remove duplicates: keep only one per duplicated entry\n\nOur data, cleanRana, has over 140 columns! That is a lot of information that we do not need. We are going to pick out only the columns we need.\n\ncleanRanaSubset <- cleanRana %>% \n  select(longitude, latitude, eventDate)\n\nLastly, we are going to write our clean data to a csv file so we can use it in later lessons.\n\nwrite.csv(cleanRanaSubset, \"ranaData.csv\")\n\nGreat job!"
  },
  {
    "objectID": "HW_2_pt1.html",
    "href": "HW_2_pt1.html",
    "title": "",
    "section": "",
    "text": "HW 2 (Part 1): Informational Interview\nHW 2 (Part 1): Informational Interview Instructions and Hints\n\n\nInstructions: Familiarize yourself with informational interviews by googling / chatGPTing ‘informational interview’. Suggested below is a youtube video to get you started too. During class on January 24, you will ask the USFWS folks the questions below in a small group setting. You will then summarize your perception of the interview, reflecting on what you learned with your personal course goals, and share with the others. \n\n\nYoutube Description of Informational Interviews: https://www.youtube.com/watch?v=m6Pa4ZB4mvQ\n\n\nPotential Informational Interview Questions:\n\n\n\nWhat did your journey look like to get to where you are today?\nWhat is your day-to-day life like at work?\nWhat do you enjoy most about your work?\nWhat is your biggest challenge at your job?\nWhat would you like to be spending more time doing at your job?\nWhat skill sets are most important in your role?\nWhat professional trends or changes should I be aware of? What didn’t you know prior to your position that you wish someone had told you?\nIf you were in my shoes, what people would you be talking with?\nCan you connect me with 3 “nouns” (people/places/or things) I should be looking into?\nStudents: come up with your own questions. What do you want to know from this expert? \n\n\n\nSummarize : Reflect on the informational interview you conducted. In 1 - 2 pages, please answer\n\nWho did you interview? \nWhat was your interviewee’s path to their current position?\nWhat is their current position like (activities, challenges, skills, etc)? Do you think you would enjoy their job? Why or why not?\nWhat was one thing that surprised you about your informational interview?\nWhat “nouns” (Q8) did your interviewee connect you with? Are you interested in them? Have you followed up with them? \nHave you ever conducted an informational interview? How was this process for you in general? \n\n\n\n\nFor those of you looking for extra challenges in this exercise\n\n\nProfessional Challenge: Do informational interviews with 2 other people the USFWS folks suggest you chat with next\n\n\nCoding Challenge: Make your informational interview summary as a markdown file"
  },
  {
    "objectID": "sdm_future.html#note-predictextent-is-defined-in-sdm_maxent_lesson.r",
    "href": "sdm_future.html#note-predictextent-is-defined-in-sdm_maxent_lesson.r",
    "title": "",
    "section": "NOTE: predictExtent is defined in sdm_maxent_lesson.R",
    "text": "NOTE: predictExtent is defined in sdm_maxent_lesson.R"
  },
  {
    "objectID": "HW_2_pt2.html",
    "href": "HW_2_pt2.html",
    "title": "",
    "section": "",
    "text": "HW 2 (Part 2): Resume\nResume Instructions and Hints\n\n\nBackground: A resume is a list of knowledge, skills, and abilities for that particular job.  Or how will you be able to do the job advertised? Thus each resume is tailored to the job posting. So the template (or where stuff goes) of the resume is the same, but the content (or the stuff) changes for each job.\n\n\nA resume is different from a CV.  A CV is the list of all your past achievements and deliverables. It is not tailored for a specific posting. The template and the content remain stable until achieving new deliverables such as a presentation, a publication, a new skill etc.\n\n\nThere are thousands of resume templates on the internet that can help you to get the position you desire; the challenge is finding the best resume template for your needs. Remember, the resume and the cover letter introduces you to potential employers and recruiters. But before you even talk with another human, an algorithm will approve your resume for human eyes to look at so make sure to use those keywords that are in the job announcement. Without those, you are unlikely to get an interview.\n\n\nYour resume should be 1) easy to read 2) easy to find keywords related to the announcement 3) be honest about your skills and abilities and 4) represent who you are as a potential colleague\n\n\nThis is for computer science students but it gives some nice examples of how to make this work for recent grads without much experience (keep scrolling on it, so many ads https://zety.com/blog/computer-science-resume). Also CrashCourse has a good series on Youtube (https://www.youtube.com/watch?v=T_4JBbeGsSE). The example provided is a bit more CV like than resume like. See if you can improve on it. \n\n\nInstructions: Write a 1 page resume for an ecological job with data science skills/tools you think you would like. Highlight what skills you need but don’t have for this type of work using a different color text. Resume should be no longer than a page (eek, we know).\n\n\nFor those of you looking for extra challenges in this exercise\n\n\nProfessional Challenge: Create this assignment with data science features (e.g. link to your GitHub, data science knowledge skills and abilities you have and/or hope to have by the end of class)\n\n\nCoding Challenge: Create this assignment in markdown"
  },
  {
    "objectID": "occ_map.html",
    "href": "occ_map.html",
    "title": "",
    "section": "",
    "text": "Occurrence Map\n\none more column to check of your data: is occurence status == PRESENT ?\n\n\n\nset x and y limits\nxmax <- max(cleanRana\\(longitude) xmin <- min(cleanRana\\)longitude) ymax <- max(cleanRana\\(latitude) ymin <- min(cleanRana\\)latitude)\n\n\nre-do map code with cleaned data and x/y limits\nggplot()+ geom_polygon(data=wrld, mapping=aes(x=long, y=lat, group=group), fill=“grey75”, colour=“grey60”)+ geom_point(data=cleanRana, mapping=aes(x=longitude, y=latitude), show.legend=FALSE)+ labs(title=“Species occurrences of R. draytonii from 1938 - 2023”, x=“longitude”, y=“latitude”) + coord_fixed(xlim = c(xmin, xmax), ylim = c(ymin, ymax)) + scale_size_area() + borders(“state”)\n\n\nmake it better by adding a date range in the title\nrange(na.omit(as.Date(cleanRana$dateIdentified)))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Info",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this Course",
    "section": "",
    "text": "Course Aims\n\n\nTarget Audience\n\n\nCourse Pre-requisites"
  },
  {
    "objectID": "sdm_current.html",
    "href": "sdm_current.html",
    "title": "",
    "section": "",
    "text": "Current SDM\n\n\nRequirements for posit.cloud\n\n\nRAM to at least 5GB\n\n\nIn project file, select NO for all fields in General\n\n\ndependencies\ninstall.packages(“dismo”) install.packages(“maptools”) install.packages(“tidyverse”) install.packages(“rJava”) install.packages(“maps”)\nlibrary(dismo) library(maptools) library(tidyverse) library(rJava) library(maps)\n\nSection 1: Obtaining and Formatting Occurence / Climate Data\n\n\n\nread occurrence data (stop to talk about camelCase)\nranaData <- read_csv(“ranaData.csv”) ranaDataNotCoords <- ranaData %>% select(longitude, latitude)\n\n\nconvert to spatial points, necessary for modelling and mapping\nranaDataSpatialPts <- SpatialPoints(ranaDataNotCoords, proj4string = CRS(“+proj=longlat”))\n\n\nobtain climate data: use get data only once\ngetData(“worldclim”, var=“bio”, res=2.5) # current data # see what each variable is here: https://www.worldclim.org/data/bioclim.html#:~:text=The%20bioclimatic%20variables%20represent%20annual,the%20wet%20and%20dry%20quarters). #?raster::getData\n\n\ncreate a list of the files in wc2-5 filder so we can make a raster stack\n\n\ndefine raster: https://desktop.arcgis.com/en/arcmap/latest/manage-data/raster-and-images/what-is-raster-data.htm\n\n\nraster consists of a matrix of cells (or pixels) organized into rows and columns\n\n\n(or a grid) where each cell contains a value representing information, such as\n\n\ntemperature. Rasters are digital aerial photographs, imagery from satellites,\n\n\ndigital pictures, or even scanned maps.\nclimList <- list.files(path = “wc2-5/”, pattern = “.bil$”, full.names = T) # ‘..’ leads to the path above the folder where the .rmd file is located\n\n\nstacking the bioclim variables to process them at one go\nclim <- raster::stack(climList)\nplot(clim[[3]]) plot(ranaDataSpatialPts, add = T)\n\nSection 2: Adding Pseudo-Absence Points\n\n\n\nCreate pseudo-absence points (making them up, using ‘background’ approach)\n\n\nfirst we need a raster layer to make the points up on, just picking 1\nmask <- raster(clim[[1]]) # mask is the raster object that determines the area where we are generating pts\n\n\ndetermine geographic extent of our data (so we generate random points reasonably nearby)\ngeographicExtent <- extent(x = ranaDataSpatialPts)\n\n\nRandom points for background (same number as our observed points we will use )\nset.seed(7536) # seed set so we get the same background points each time we run this code backgroundPoints <- randomPoints(mask = mask, n = nrow(ranaDataNotCoords), # n should be same n as in the pts to be used to test ext = geographicExtent, extf = 1.25, # draw a slightly larger area than where our sp was found (ask katy what is appropriate here) warn = 0) # don’t complain about not having a coordinate reference system\n\n\nadd col names (can click and see right now they are x and y)\ncolnames(backgroundPoints) <- c(“longitude”, “latitude”)\n\nSection 3: Collate Env Data and Point Data into Proper Model Formats\n\n\n\nData for observation sites (presence and background), with climate data\noccEnv <- na.omit(raster::extract(x = clim, y = ranaDataNotCoords)) absenceEnv<- na.omit(raster::extract(x = clim, y = backgroundPoints)) # again, many NA values\n\n\nCreate data frame with presence training data and backround points (0 = abs, 1 = pres)\npresenceAbsenceV <- c(rep(1, nrow(occEnv)), rep(0, nrow(absenceEnv))) presenceAbsenceEnvDf <- as.data.frame(rbind(occEnv, absenceEnv))\n\nSection 4: Create SDM with Maxent\n\n\n\ncreate a new folder called maxent_outputs\nranaSDM <- dismo::maxent(x = presenceAbsenceEnvDf, # env conditions p = presenceAbsenceV, # p = 1, a = 0 path = “maxent_outputs” )\n\n\nview the maxent model by navigating in maxent_outputs folder for the html\nresponse(ranaSDM) ### Section 5: Plot the Model ### # clim is huge and it isn’t reasonable to predict over whole world # first we will make it smaller predictExtent <- 1.25 * geographicExtent geographicArea <- crop(clim, predictExtent) # crop clim data to predict extent\nranaPredictPlot <- raster::predict(ranaSDM, geographicArea)\n\n\nfor ggplot, we need the prediction to be a data frame\nraster.spdf <- as(ranaPredictPlot, “SpatialPixelsDataFrame”) ranaPredictDf <- as.data.frame(raster.spdf)\n\n\nplot in ggplot\nwrld <- ggplot2::map_data(“world”)\nxmax <- max(ranaPredictDf\\(x) xmin <- min(ranaPredictDf\\)x) ymax <- max(ranaPredictDf\\(y) ymin <- min(ranaPredictDf\\)y)\nggplot() + geom_polygon(data = wrld, aes(x = long, y = lat, group = group), fill = “grey”) + geom_raster(data = ranaPredictDf, aes(x = x, y = y, fill = layer)) + scale_fill_gradientn(colors = terrain.colors(10, rev = T)) + coord_fixed(xlim = c(xmin, xmax), ylim = c(ymin, ymax), expand = F) + borders(“state”) + geom_point(data = ranaDataNotCoords, aes(x = longitude, y = latitude, alpha = 0.5))"
  },
  {
    "objectID": "HW_6.html",
    "href": "HW_6.html",
    "title": "",
    "section": "",
    "text": "HW 6: Current & Future SDM\nSpecies Distribution Modeling Instructions\n\n\nUsing the code from class, create \n\nA current species distribution of your focal SSA species\nA future species distribution (70 years from now) of your focal SSA species \n\n\n\nEach data visualization of the model needs to include\n\nA descriptive title\nAxes labels\nClear legend \nCountry and state borders\n\n\n\nEach person needs to turn in\n\nAn image of your current species distribution of your focal species\nAn image of your future species distribution (70 years from now) of your focal species\nThe .R file you used to create those images\n\n\n\n\nFor those of you looking for extra challenges in this exercise\n\n\nProfessional Challenge: Add a paragraph explaining this figure similar to what you would see in an SSA publication. E.g. Figure 2: Maxent Species Distribution Model of the Foothill Yellow-Legged Frog (Rana boylii). This species is mostly detected on coastal California however suitable habitat extends into the central valley. Occurrence data was downloaded from GBIF on 02/03/2023 (doi:) and available on GitHub (link).  \n\n\nCoding Challenge: Transfer your .R  to a .md with instructions ready to be published on GitPages for others to learn how you did your model."
  },
  {
    "objectID": "HW_4.html",
    "href": "HW_4.html",
    "title": "",
    "section": "",
    "text": "HW 4: Occurrence Map\n2023 Occurrence Map Instructions\n\n\nUsing the species your group decided on, each person needs to turn in a .R file in which you\n\nObtain data\n\nFrom  GBIF\nSet limit = 4000\n\nClean data \n\nRemove duplicates\nRemove where lat/long are NA\nRemove points where occurrence status is not present\nplaces that are not logical… be sure to justify with comments what points you determined were not logical ie. this species is endemic to Oregon so I can remove points in Australia\n\nCreate a map of the occurrence points\n\nAxes labels\nBorders of countries and states\nDescriptive title\n\n\n\n\nUsing comments, write a sentence or two describing what the occurrence points are. \n\n\nYou will have class time on Tuesday Feb 14 to work on this."
  },
  {
    "objectID": "HW_5.html",
    "href": "HW_5.html",
    "title": "",
    "section": "",
    "text": "HW 5: Natural History and Taxonomy\nNatural History Information Instructions\n\n\nFill in the following subsections for your group’s chosen species. Each person will submit their own work. Later when making the final document for USFWS, you will aggregate and edit everyone’s information into one document. This homework should be a minimum of 1 page single spaced. (Note - interpret “pp” below as paragraphs, not pages for your answers to this.)\n\n\nSee SSA for CactusFerruginousPygmyOwl https://ecos.fws.gov/ServCat/DownloadFile/211651 for what this looks like in the final report. We are aiming for your sections to be included in the document, but not provide all of the content (USFWS needs to make a lot of content!). \n\n\nTaxonomy: (1-2 pp) What is this species referred to in the literature and how confident are we that it isn’t a different species? E.g. species name, taxonomic affiliations, systematic relationships (ie close relatives), species status (ie how certain are we about the taxonomy)\n\n\nKey Identification Characteristics: (1 pp) what characteristics do you use to id this species\n\n\nHabitats: (1-2 pp) What kinds of habitats does your species affiliate with (ie riparian, alpine, thorn scrub, etc)\n\n\nHistoric Range: (1 pp) Where is the species thought to have occurred in the last 50 years 9e.g. Western North America, Central North America, Northern Florida etc, etc)\n\n\nReferences: Please reference your information as best you can including primary sources whenever possible. Again this makes it easier for USFWS to make their final SSA report.\n\n\nFor those of you looking for extra challenges in this exercise\n\n\nProfessional Challenge: Add another section on key life history and/or population movement components you think will be useful for determining status of this species under the 3 Rs (Resiliency, Representation, and Redundancy)\n\n\nCoding Challenge:Make your informational interview summary as a markdown file ready to be published on GitPages\n\n\n\nPp = paragraph"
  },
  {
    "objectID": "HW_1.html",
    "href": "HW_1.html",
    "title": "",
    "section": "",
    "text": "HW 1: Data Science Tools\nHW 1: Data Science Tools Instructions\nPurpose: The goal of this homework is to introduce you to the data science tools we will use in this class. We do not expect you to come into this class with this knowledge. Some of these tools will likely be new to everyone.\nBackground: Data Science is an ever changing assortment of tools and approaches. In this course, we will use a variety of them to create a report on a species that is being evaluated for endangered species status. You will learn the natural history of a species and use data to create maps related to where the species has been found and where it could be found given the same environmental conditions using models. You will then share your report with the world on the internet using some of the tools listed below.  \n\nThis homework will introduce you to/ remind you of the following programs that are commonly used in ecological research to create projects like the one you will do this semester. All of these are essential tools in ecological and conservation research, for now. \n\nR Language for Programming\nR packages\nR Studio\nGit\nGitHub\nR Markdown\nZenodo \n\nTo Do: In a couple paragraphs for each tool (~2-4 page single-spaced total), explain what each of the above tools is, and what it is primarily used for in data science. Of course you may Google these, but describe them in your own words. For each of these tools write a sentence or two about how they might be used in a project applied to conservation, like the one you will create this semester.\n\n\nFeel free to use headers and subheaders to organize your work. For example, \nR Programming Language (https://www.r-project.org)\nWhat is R? \nR is an open source language and environment for statistical computing and graphics derived from the S language and environment developed in Bell Laboratories (AT&T) to turn ideas into software, quickly, and faithfully. Etc, etc, etc\n\nWhy use R for statistical computing and graphics?\nR provides a transparent and reproducible way to analyze and visualize data \n\nHow and why will I use R in my research project / life ?\nFor everything! All the R. I use it to calculate my taxes and weekly budget\n\n\nFinally, please create a GitHub account.\n\n\nFor those of you looking for extra challenges in this exercise\n\n\nProfessional Challenge: Identify 2 new skills from this exercise that you would now like in your course personal goals\n\n\nCoding Challenge: Create this assignment in markdown \n\n\nTo Complete this Homework:\n\nL&C students upload your document in Moodle\nUA students upload your document into D2L\nUpdate the google sheet below with your GitHub and Slack Username https://docs.google.com/spreadsheets/d/1ooLI9HfDxCiuGlLvv8L5sxLxbeWj6d5IGIapKAPUdA4/edit?usp=sharing\nIf you have not joined Slack (in class activity), please do so here."
  },
  {
    "objectID": "for_students.html",
    "href": "for_students.html",
    "title": "Class Schedule",
    "section": "",
    "text": "Week\nDate\nTopics during class time\nAssignments due\nImportant Notes\n\n\n1\nJan 10\nClasses start UAZ, no class this week\nNA\nUAZ starts on Wed\n\n\n2\nJan 17\nIntroductions, surveys, creating github account, and preparing for stakeholder meeting\nIn class surveys, In class GitHub registration\nFirst day of class for both schools, grant evaluation survey, getting to know you survey\n\n\n3\nJan 24\nEndangered species act lecture, meet USFWS collaborators, and stakeholder interviews\nHW 1 - Data science tools homework\nPreclass reading on USFWS and ESA; Stakeholder visit with time for informational interviews. HW 1 due.\n\n\n4\nJan 31\nGroup assignments, R studio, github, and project management\nHW 2 - One page resume, summary from informational interviews, Personal course goals\nAssign teams, Preclass reading on consulting/project management basics. HW 2 due.\n\n\n5\nFeb 7\nData acquisition, cleaning, and visualization using GBIF\nHW 3 - GitHub Repo with ReadMe for each group\nPreclass reading on coding good enough practices and project management in github. HW 3 due.\n\n\n6\nFeb 14\nProject group work. Focus species data acquisition, cleaning, and mapping\nHW 4 - Occurrence map\nPreclass reading on occurrence data wrangling. HW 4 due.\n\n\n7\nFeb 21\nSpecies distribution models (SDMs)\nHW 5 - Natural history/taxonomy summary\nPreclass reading on SDMs. HW 5 due.\n\n\n8\nFeb 28\nFuture SDM in class lesson. Time to discuss deliverables and work flow as a team (time to start updating repos). In class work time for your focal species current and future SDM.\nHW 6 - SDMs\nHW 6 due Friday at midnight.\n\n\n9\nMar 7\nLC meets only. Focus species project group work on organizing GH repo and materials for sharing with stakeholders.\n\n\nUA Spring Break\n\n\n10\nMar 14\nProject group work on presentation and GH repo to share with stakeholders.\n\n\nPreclass reading on giving and receiving feedback\n\n\n11\nMar 21\nReview group progress. Give a short presentation in class on your species. Each stakeholder will be assigned to 1-2 groups and evaluate deliverables (e.g. taxonomy review, occurrence maps, SDMs) and GitHub Repo\nPresentation to stakeholders (GH repo and info on your sp)\n2nd USFWS stakeholder visit\n\n\n12\nMar 28\nUA meets only, Project group work re stakeholder feedback\n\n\nLC Spring Break\n\n\n13\nApr 4\nGroup work re stakeholder feedback\n\n\n\n\n\n\n14\nApr 11\nGroup work for final presentations\n\n\n\n\n\n\n15\nApr 18\nStakeholder visit: showcasing student deliverables. Listen and evaluate student presentation and GitHub Repo\nFinal presentation to stakeholders\nFinal stakeholder visit\n\n\n16\nApr 25\nSubmit final project. Reflection and evaluation.\nFinal project submission on github, in-class reflection and survey\nLast day of class for both institutions, recap and survey in class. Good luck with finals!"
  },
  {
    "objectID": "for_students.html#reading-schedule",
    "href": "for_students.html#reading-schedule",
    "title": "Class Schedule",
    "section": "Reading Schedule",
    "text": "Reading Schedule\n\n\n\nRead by class on\nLinks\nReading Questions\n\n\nJan 24\nWhat is a Species Status Assessment (SSA)?\nThe Endangered Species Act (ESA)\nEcological Services provided by the US Fish and Wildlife Service\nWhat US Fish and Wildlife Service does with the ESA\n\n\n\nJan 31\nR (programming language)\nIDE\nThe Center for Open Science\nUNESCO Recommendations for Open Science\n\n\n\nFeb 7\nData cleaning\niNaturalist and Scientific Discoveries - read through Data Validation\nUses of GBIF in conservation  - watch the video\n\n\n\nFeb 14\nScientific article on public database accuracy for SDMs - case study, read introduction and discussion\nAbstract from Biodiversity Information Science and Standards 2022 - conference proceedings abstract\nUSGS Wildlife wrangler GitLab - read ReadMe.md especially Valuable Features\n\n\n\nFeb 21\nEcological niche\nGeneral intro to the concept of SDM\nMore detailed slide deck about SDM\nAbout MaxEnt, a type of SDM\n\n\n\nFeb 28\n\n\n\n\n\nMarch 7\n\n\n\n\n\nMarch 14\n\n\n\n\n\nMarch 21\n\n\n\n\n\nMarch 28\n\n\n\n\n\nApril 4\n\n\n\n\n\nApril 11\n\n\n\n\n\nApril 18"
  },
  {
    "objectID": "coding_basics_for_this_course.html",
    "href": "coding_basics_for_this_course.html",
    "title": "",
    "section": "",
    "text": "Coding Basics\nPrerequisites for this course:\nSkills necessary:\nBuilding or reviewing coding skills"
  },
  {
    "objectID": "HW_3.html",
    "href": "HW_3.html",
    "title": "",
    "section": "",
    "text": "HW 3: Practice with GitHub\nNote: It is critical you do these steps in the exact order outlined below.\n\n\nGetting your group repository onto your posit cloud\n\n\n\nLog in to your personal GitHub account\nNavigate to our class’ organization, BiodiversityDataScienceCorp\nFind the repository that has your group name on it\nClick the green code button and copy the link there\nLog in to your posit cloud account\nCreate a new project: new project from Git Repository\nPaste the link you copied from GitHub when prompted\n\n\n\nYou now should have a project in posit.cloud that is your local copy of your group’s repository. This project should already be populated with a gitignore, license, and README file. There should be a Git tab in the top right corner of R studio.\n\n\nSetting up your group repository for commits/pushes/pulls\n\nIn this new project in rstudio cloud, add a line in the gitignore file that says “github.R”\nCreate a new .R file and name it “github.R”\nCopy the contents from the github.R file you have in the project you created during class\nPaste them to the new github.R file in your group repository. Save it. Highlight all of the code and run it. \n\n\n\nYou now should be able to commit/push/pull to/from GitHub to work collaboratively with your group. \n\n\nTo work collaboratively with GitHub / R studio, follow this flow\n*Only one person edits the same file at a time. If two people edit the same document simultaneously, you will have to do extra work to ensure all of your saves are committed properly. \n*Multiple people can work at once on different files. For example, Person A can edit your README while Person B edits your gitignore. \n\nPerson A: Write some code in a file and click save when you are done\nPerson A: Commit and push the code. Leave a detailed commit message for your future self if you ever need to go back to a previous version. Be sure to push. \nPerson B: Pull first!!! \nPerson B: Edit the file. Save. Commit. Push. \nPerson A: Pull. Etc. You now have a rhythm for working on the same file. \n\n\n\nSetting up your repository’s README file\n\n\nREADMEs are written in markdown. See this markdown cheat sheet for a guide on how to create headings, change the font, etc. \n\n\nYour README file needs to contain, at the least: \n\nYour group members’ names\nYour species\nHave each group member write one paragraph about their favorite species (and commit/push/pull themselves). Remember to take turns. \n\n\n\nTips\n\nAlways save, commit, and push after working on a file\nAlways pull before working on a file\nGive your commit messages descriptions that would help you if you needed to “undo” to go back to a previous version\nCommunicate with your group when you plan to work on a file so two people are not working on the same file at once\n\n\n\n\nFor those of you looking for extra challenges in this exercise\n\n\nProfessional Challenge:Add your github information to your resume\n\n\nCoding Challenge:  Add a photo of your species to your README file. You’ll need to upload the photo to your repo, then you can use markdown syntax to call the photo."
  },
  {
    "objectID": "sdm_future.html",
    "href": "sdm_future.html",
    "title": "",
    "section": "",
    "text": "future SDM\nusing CMIP5\nInstall packages & load libraries IFF they aren’t yet part of your project environment\nThey may have already been loaded in sdm_maxent_lesson.R.\npackages\ninstall.packages(“dismo”) install.packages(“maptools”) install.packages(“tidyverse”) install.packages(“rJava”) install.packages(“maps”)\nlibrary(dismo) library(maptools) library(tidyverse) library(rJava) library(maps)"
  },
  {
    "objectID": "reading_schedule.html",
    "href": "reading_schedule.html",
    "title": "",
    "section": "",
    "text": "Read by class on\nLinks\nReading Questions\n\n\nJan 24\nWhat is a Species Status Assessment (SSA)?\nThe Endangered Species Act (ESA)\nEcological Services provided by the US Fish and Wildlife Service\nWhat US Fish and Wildlife Service does with the ESA\n\n\n\nJan 31\nR (programming language)\nIDE\nThe Center for Open Science\nUNESCO Recommendations for Open Science\n\n\n\nFeb 7\nData cleaning\niNaturalist and Scientific Discoveries - read through Data Validation\nUses of GBIF in conservation  - watch the video\n\n\n\nFeb 14\nScientific article on public database accuracy for SDMs - case study, read introduction and discussion\nAbstract from Biodiversity Information Science and Standards 2022 - conference proceedings abstract\nUSGS Wildlife wrangler GitLab - read ReadMe.md especially Valuable Features\n\n\n\nFeb 21\nEcological niche\nGeneral intro to the concept of SDM\nMore detailed slide deck about SDM\nAbout MaxEnt, a type of SDM\n\n\n\nFeb 28\n\n\n\n\n\nMarch 7\n\n\n\n\n\nMarch 14\n\n\n\n\n\nMarch 21\n\n\n\n\n\nMarch 28\n\n\n\n\n\nApril 4\n\n\n\n\n\nApril 11\n\n\n\n\n\nApril 18"
  }
]